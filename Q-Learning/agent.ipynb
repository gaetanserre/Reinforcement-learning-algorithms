{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-Learning simple example\n",
    "Guide from [valohai](https://valohai.com/blog/reinforcement-learning-tutorial-part-1-q-learning/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from hanoi import Hanoi, Action\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table_value (table, action, state):\n",
    "  return table[action][state] if state in table[action] else 0\n",
    "\n",
    "def choose_action(d, table):\n",
    "  actions = d.get_actions()\n",
    "  values = list(map(lambda a: get_table_value(table, a, d.state), actions))\n",
    "\n",
    "  if values.count(values[0])==len(values):\n",
    "    return Action(np.random.choice(actions))\n",
    "  else:\n",
    "    idx_max = max(range(len(values)), key=values.__getitem__)\n",
    "    return actions[idx_max]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(nb_actions):\n",
    "  table = []\n",
    "  for _ in range(nb_actions):\n",
    "    table.append({})\n",
    "  return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_fit(nb_iterations=2000):\n",
    "  d = create_env()\n",
    "  greedy_table = create_table(env.nb_actions)\n",
    "  for _ in range(nb_iterations):\n",
    "    action = choose_action(d, greedy_table)\n",
    "    old_state = d.state\n",
    "    d.set_state(d.get_new_state(action))\n",
    "    reward = d.get_reward()\n",
    "    greedy_table[action][old_state] = get_table_value(greedy_table, action, old_state) + reward\n",
    "\n",
    "    if d.is_final_state():\n",
    "      d = create_env()\n",
    "  return greedy_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_q_action(d, q_table, gambling_rate):\n",
    "  if np.random.random() <= gambling_rate:\n",
    "    return Action(np.random.choice(d.get_actions()))\n",
    "  else:\n",
    "    return choose_action(d, q_table)\n",
    "\n",
    "def update_q_table(eta, discount, q_table, old_state, d_new, action):\n",
    "  reward = d_new.get_reward()\n",
    "  actual_value = get_table_value(q_table, action, old_state)\n",
    "\n",
    "  max_list = list(map(lambda a: get_table_value(q_table, a, d_new.state), d_new.get_actions()))\n",
    "  max_value = max(max_list)\n",
    "  \n",
    "  return actual_value + eta * (reward + discount * max_value - actual_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Q_fit(nb_iterations=4000):\n",
    "  d = create_env()\n",
    "  eta = 0.1\n",
    "  discount = 0.95\n",
    "  gambling_rate = 1\n",
    "\n",
    "  q_table = create_table(env.nb_actions)\n",
    "  for _ in range(nb_iterations):\n",
    "    action = choose_q_action(d, q_table, gambling_rate)\n",
    "    gambling_rate -= 1/nb_iterations\n",
    "    old_state = d.state\n",
    "    d.set_state(d.get_new_state(action))\n",
    "    q_table[action][old_state] = update_q_table(eta, discount, q_table, old_state, d, action)\n",
    "\n",
    "    if d.is_final_state():\n",
    "      d = create_env()\n",
    "  return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play(policy):\n",
    "  d = create_env()\n",
    "  nb_moves = 0\n",
    "  while d.state != d.final_state and nb_moves < 100:\n",
    "    action = choose_action(d, policy)\n",
    "    d.set_state(d.get_new_state(action))\n",
    "    nb_moves += 1\n",
    "  return nb_moves\n",
    "\n",
    "def plot_Q_evolution(it_list):\n",
    "  plt.figure(dpi=150)\n",
    "  nb_optimal_moves = 2**N-1\n",
    "  nb_moves = []\n",
    "  for it in it_list:\n",
    "    tmp = []\n",
    "    for _ in range(5):\n",
    "      policy = Q_fit(it)\n",
    "      tmp.append(play(policy))\n",
    "    nb_moves.append(np.mean(tmp))\n",
    "  \n",
    "  plt.plot(it_list, nb_moves, \".-\", color='r', label=\"Number of moves found by the agent\")\n",
    "  plt.axhline(nb_optimal_moves, color=\"orange\", label=f\"Optimal number of moves = {nb_optimal_moves}\")\n",
    "  plt.title(f\"Solving Towers of HanoÃ¯ with {N} disks using Q-Learning\")\n",
    "  plt.legend(bbox_to_anchor=(0.5, -0.2), loc=\"center\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Hanoi\n",
    "N = 5\n",
    "def create_env():\n",
    "  return env(N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "31"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy = Q_fit(250000)\n",
    "play(policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#greedy_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_Q_evolution(np.arange(150000, 250000, 15000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
